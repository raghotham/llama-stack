---
description: Get started with Llama Stack in minutes
sidebar_label: Quickstart
sidebar_position: 1
title: Quickstart
---

Get started with Llama Stack in minutes!

Llama Stack is a stateful service with REST APIs to support the seamless transition of AI applications across different
environments. You can build and test using a local server first and deploy to a hosted endpoint for production.

In this guide, we'll walk through how to use the **Responses API** and **Conversations API** to build AI applications
with [Ollama](https://ollama.com/) as the inference [provider](/docs/providers/inference).

**ðŸ’¡ Notebook Version:** You can also follow this quickstart guide in a Jupyter notebook format: [quick_start.ipynb](https://github.com/meta-llama/llama-stack/blob/main/docs/quick_start.ipynb)

#### Step 1: Install and Setup

1. Install [Ollama](https://ollama.com/download) and start a Llama model:
```bash
ollama run llama3.2:3b --keepalive 60m
```

2. Install [uv](https://docs.astral.sh/uv/) for dependency management.

#### Step 2: Run the Llama Stack Server

```bash
# Install dependencies for the starter distribution
uv run --with llama-stack llama stack list-deps starter | xargs -L1 uv pip install

# Run the server
OLLAMA_URL=http://localhost:11434 uv run --with llama-stack llama stack run starter
```

#### Step 3: Use the Responses API

The Responses API provides a simple interface for chat interactions. Create a file `demo.py`:

```python
from openai import OpenAI

# Connect to Llama Stack server
client = OpenAI(base_url="http://localhost:8321/v1/", api_key="none")

# Simple chat using Responses API
response = client.responses.create(
    model="ollama/llama3.2:3b",
    input="What are three tips for writing clean code?"
)

print(response.output[-1].content[-1].text)
```

Run with:
```bash
uv run --with openai python demo.py
```

#### Step 4: Multi-turn Conversations

For multi-turn conversations, use the Conversations API:

```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8321/v1/", api_key="none")

# Create a persistent conversation
conversation = client.conversations.create(metadata={"topic": "intro"})

# First turn - response is automatically added to conversation
response1 = client.responses.create(
    model="ollama/llama3.2:3b",
    input="My name is Alice. Remember that.",
    conversation=conversation.id,
)
print("Assistant:", response1.output[-1].content[-1].text)

# Second turn - conversation history is automatically loaded
response2 = client.responses.create(
    model="ollama/llama3.2:3b",
    input="What's my name?",
    conversation=conversation.id,
)
print("Assistant:", response2.output[-1].content[-1].text)
```

Congratulations! You've successfully built your first Llama Stack application! ðŸŽ‰

### Next Steps

Now you're ready to dive deeper into Llama Stack!
- Explore the [Detailed Tutorial](./detailed_tutorial) for RAG agents and more advanced examples.
- Try the [Getting Started Notebook](https://github.com/meta-llama/llama-stack/blob/main/docs/getting_started.ipynb).
- Browse more [Notebooks on GitHub](https://github.com/meta-llama/llama-stack/tree/main/docs/notebooks).
- Learn about Llama Stack [Concepts](/docs/concepts).
- Discover how to [Build Llama Stacks](/docs/distributions).
- Refer to our [References](/docs/references) for details on the Llama CLI and Python SDK.
- Check out the [llama-stack-apps](https://github.com/meta-llama/llama-stack-apps/tree/main/examples) repository for example applications and tutorials.
